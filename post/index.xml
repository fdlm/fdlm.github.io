<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on fdlm.github.io</title>
    <link>https://fdlm.github.io/post/</link>
    <description>Recent content in Posts on fdlm.github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Licensed under a &lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;</copyright>
    <lastBuildDate>Fri, 22 Dec 2017 10:37:55 +0100</lastBuildDate>
    
	<atom:link href="https://fdlm.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Experiments with AMSGrad</title>
      <link>https://fdlm.github.io/post/amsgrad/</link>
      <pubDate>Fri, 22 Dec 2017 10:37:55 +0100</pubDate>
      
      <guid>https://fdlm.github.io/post/amsgrad/</guid>
      <description>For ICLR 2018, two papers targeting problems with the ADAM update rule were submitted: On the Convergence of Adam and Beyond, and Fixing Weight Decay Regularization in Adam. The former points to a flaw in ADAM&amp;rsquo;s proof of convergence, and provides a simple solution. The resulting algorithm is called AMSGrad. The paper shows the advantages of AMSGrad on a synthetic task and a small number of experiments. However, it only uses small networks (1-layer MLP on MNIST, and a small ConvNet for CIFAR-10), and does not show test accuracy, the metric we ultimately care about more than cross-entropy loss.</description>
    </item>
    
    <item>
      <title>A Fully Convolutional Deep Auditory Model for Musical Chord Recognition</title>
      <link>https://fdlm.github.io/post/auditorymodel/</link>
      <pubDate>Sun, 24 Jul 2016 08:38:14 +0200</pubDate>
      
      <guid>https://fdlm.github.io/post/auditorymodel/</guid>
      <description>This site contains information on reproducing the experiments in the paper
A Fully Convolutional Deep Auditory Model for Musical Chord Recognition
Korzeniowski, F. and Widmer, G.
In Proceedings of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP), Salerno, Italy, 2016.
Software The pre-trained chord recognition model is available as part of the madmom audio processing framework. It differs only slightly from the model presented in the paper: instead of padding the input in the first few convolutional layers, we use a larger context window in the input layer, such that the feature map size after the first pooling layer is retained.</description>
    </item>
    
    <item>
      <title>The Deep Chroma Extractor</title>
      <link>https://fdlm.github.io/post/deepchroma/</link>
      <pubDate>Mon, 23 May 2016 09:22:08 +0200</pubDate>
      
      <guid>https://fdlm.github.io/post/deepchroma/</guid>
      <description>This page contains additional information and data necessary to reproduce the results of the following paper:
F. Korzeniowski and G. Widmer. &amp;ldquo;Feature Learning for Chord Recognition: The Deep Chroma Extractor&amp;rdquo;. In Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR 2016), New York, USA.
Software The pre-trained feature extractor is available as part of the madmom audio processing framework as the DeepChromaProcessor class. Note that the model bundled with madmom differs from the one we used in the paper: it uses less units in the hidden layers of the neural network and operates on a narrower frequency band.</description>
    </item>
    
  </channel>
</rss>